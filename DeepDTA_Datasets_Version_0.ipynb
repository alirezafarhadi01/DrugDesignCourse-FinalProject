{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezafarhadi01/DrugDesignCourse-FinalProject/blob/main/DeepDTA_Datasets_Version_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the path to your zip file\n",
        "zip_file_path = '/content/data.zip' # Replace with the actual path to your .zip file\n",
        "\n",
        "# Define the directory where you want to extract the contents\n",
        "# If the directory doesn't exist, it will be created.\n",
        "extract_dir = '/content/' # You can change this to your desired extraction path\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"Successfully extracted '{zip_file_path}' to '{extract_dir}'\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{zip_file_path}' was not found.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: '{zip_file_path}' is not a valid zip file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Optional: List the contents of the extracted directory to verify\n",
        "print(\"\\nContents of the extracted directory:\")\n",
        "for item in os.listdir(extract_dir):\n",
        "    print(os.path.join(extract_dir, item))"
      ],
      "metadata": {
        "id": "plZOvqDQDW7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ccc998-065c-4d6b-c699-84de78f65625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted '/content/data.zip' to '/content/'\n",
            "\n",
            "Contents of the extracted directory:\n",
            "/content/.config\n",
            "/content/data.zip\n",
            "/content/kiba\n",
            "/content/davis\n",
            "/content/sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, numpy as np, pickle, gzip\n",
        "\n",
        "# candidates for Y path\n",
        "candidate_paths = [\n",
        "    \"/content/kiba/Y\",\n",
        "    \"/content/kiba/Y.npy\",\n",
        "    \"/content/kiba/Y.pkl\",\n",
        "    \"/content/kiba/Y.pickle\",\n",
        "    \"/content/kiba/Y.npz\",\n",
        "    \"/content/kiba/Y.gz\",\n",
        "]\n",
        "\n",
        "def first_existing(paths):\n",
        "    for p in paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    raise FileNotFoundError(\"Could not find Y file in expected locations.\")\n",
        "\n",
        "def load_affinity_matrix(path):\n",
        "    # 1) numpy loader (handles .npy/.npz and sometimes raw)\n",
        "    try:\n",
        "        obj = np.load(path, allow_pickle=True)\n",
        "        if isinstance(obj, np.lib.npyio.NpzFile):\n",
        "            if 'arr_0' in obj.files:\n",
        "                return obj['arr_0']\n",
        "            # fallback to the first key\n",
        "            return obj[obj.files[0]]\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 2) pickle default\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 3) pickle with latin1 (py2 pickles)\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f, encoding=\"latin1\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 4) gzip+pickle\n",
        "    try:\n",
        "        with gzip.open(path, \"rb\") as f:\n",
        "            return pickle.load(f, encoding=\"latin1\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to load affinity matrix from {path}: {e}\")\n",
        "\n",
        "y_path = first_existing(candidate_paths)\n",
        "affinity_matrix = load_affinity_matrix(y_path)\n",
        "if not isinstance(affinity_matrix, np.ndarray):\n",
        "    affinity_matrix = np.array(affinity_matrix)\n",
        "if affinity_matrix.ndim != 2:\n",
        "    raise ValueError(f\"Unexpected affinity matrix shape: {affinity_matrix.shape}\")\n",
        "\n",
        "valid_indices = np.where(~np.isnan(affinity_matrix))\n",
        "total_valid_pairs = int(len(valid_indices[0]))\n",
        "print(f\"Detected Y at: {y_path}\")\n",
        "print(f\"Total valid pairs: {total_valid_pairs}\")\n",
        "\n",
        "# save split under /content/New Data\n",
        "save_dir = \"/content/New Data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "split_path = os.path.join(save_dir, \"kiba_split.json\")\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "all_indices = np.arange(total_valid_pairs)\n",
        "rng.shuffle(all_indices)\n",
        "split_point = int(0.8 * total_valid_pairs)\n",
        "train_indices = all_indices[:split_point]\n",
        "test_indices  = all_indices[split_point:]\n",
        "\n",
        "split = {\n",
        "    \"train_indices\": train_indices.tolist(),\n",
        "    \"test_indices\": test_indices.tolist(),\n",
        "    \"total_valid_pairs\": total_valid_pairs,\n",
        "    \"seed\": 42,\n",
        "    \"y_path_used\": y_path,\n",
        "}\n",
        "with open(split_path, \"w\") as f:\n",
        "    json.dump(split, f)\n",
        "\n",
        "print(f\"Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n",
        "print(f\"Saved JSON: {split_path}\")\n"
      ],
      "metadata": {
        "id": "Y9u1kVWmASv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94893d6-183b-4a12-8c9c-24b8e06357eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Y at: /content/kiba/Y\n",
            "Total valid pairs: 118254\n",
            "Train size: 94603, Test size: 23651\n",
            "Saved JSON: /content/New Data/kiba_split.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, numpy as np, os\n",
        "\n",
        "split_path = \"/content/New Data/kiba_split.json\"\n",
        "with open(split_path, \"r\") as f:\n",
        "    split = json.load(f)\n",
        "\n",
        "train_indices = np.array(split[\"train_indices\"], dtype=int)\n",
        "test_indices  = np.array(split[\"test_indices\"], dtype=int)\n",
        "\n",
        "print(f\"Loaded split -> Train: {len(train_indices)}, Test: {len(test_indices)}\")\n"
      ],
      "metadata": {
        "id": "ghd-mdW6iwJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a6595d-9c62-498d-ad7f-9e00814b3a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded split -> Train: 94603, Test: 23651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWTZOdu6gzlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eQckmHvg7QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gva0kLxqsW80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D75uIF_-hB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ushrKscLhEgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nNyagAxGiVsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}